# -*- coding: utf-8 -*-
"""Lyra_DO_Vert_7B.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qH7e_wehJT_fpaRpSLpZkoe3K5C0hXK1
"""

# Bloc 1 â€” Install & vÃ©rification des fichiers JSONL
!pip install -q --upgrade "transformers>=4.46.0" peft datasets trl accelerate huggingface_hub

import transformers
import trl
import torch

print("transformers version :", transformers.__version__)
print("trl version          :", trl.__version__)
print("torch version        :", torch.__version__)
print("CUDA dispo:", torch.cuda.is_available())

import os, json

# ğŸ”§ Adapter les chemins 
train_path = "/content/sample_data/train_tomate_azote_DO_pH_1000.jsonl"
valid_path = "/content/sample_data/eval_tomate_azote_DO_pH_20.jsonl"  # petit jeu d'Ã©val manuel

def check_jsonl(path, n=3):
    print(f"\nğŸ” VÃ©rification du fichier : {path}")
    if not os.path.exists(path):
        print("âŒ Fichier introuvable.")
        return False
    try:
        with open(path, "r", encoding="utf-8") as f:
            lines = [next(f).strip() for _ in range(n)]
        print(f"âœ… Fichier trouvÃ© ({os.path.getsize(path)/1024:.1f} Ko)")
        for i, line in enumerate(lines, 1):
            data = json.loads(line)
            print(f"â€” Ligne {i} OK, clÃ©s principales :", list(data.keys()))
        print("âœ… Structure JSONL valide (au moins sur les 3 premiÃ¨res lignes).")
        return True
    except Exception as e:
        print("âš ï¸ Erreur :", e)
        return False

ok_train = check_jsonl(train_path)
ok_valid = check_jsonl(valid_path)

if ok_train and ok_valid:
    print("\nğŸš€ Les deux datasets sont valides et prÃªts Ã  Ãªtre chargÃ©s.")
else:
    print("\nâš ï¸ ProblÃ¨me dÃ©tectÃ© sur au moins un des fichiers.")

# Bloc 2 â€” Login Hugging Face avec TOKEN_DO_Vert
import torch, os
from google.colab import userdata
from huggingface_hub import login

base_model_id = "mistralai/Mistral-7B-Instruct-v0.3"

print("GPU :", torch.cuda.get_device_name(0))
print("CUDA dispo:", torch.cuda.is_available())

# ğŸ” secret Colab TOKEN_DO_Vert
hf_token = userdata.get('TOKEN_DO_Vert')
if not hf_token:
    raise ValueError("âŒ Le secret TOKEN_DO_Vert est introuvable ou vide.")
os.environ["HF_TOKEN"] = hf_token

login(token=os.environ["HF_TOKEN"])
print("ğŸ” Connexion Hugging Face OK")

# Bloc 3 â€” Chargement du JSONL & application chat template
import json
from datasets import Dataset, DatasetDict
from transformers import AutoTokenizer

def load_jsonl(path):
    rows = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            rows.append(json.loads(line))
    return rows

train_rows = load_jsonl(train_path)
valid_rows = load_jsonl(valid_path)

dataset = DatasetDict({
    "train": Dataset.from_list(train_rows),
    "validation": Dataset.from_list(valid_rows)
})

print("taille train :", len(dataset["train"]))
print("taille valid :", len(dataset["validation"]))
print("exemple brut :", dataset["train"][0])

tokenizer = AutoTokenizer.from_pretrained(base_model_id, use_fast=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

def to_chat_text(example):
    # On prend tel quel le tableau "messages" fourni dans le dataset DO_vert
    txt = tokenizer.apply_chat_template(
        example["messages"],
        tokenize=False,
        add_generation_prompt=False,
    )
    return {"text": txt}

dataset = dataset.map(to_chat_text)

print("aperÃ§u text :", dataset["train"][0]["text"][:500])

print("transformers version :", transformers.__version__)
print("trl version          :", trl.__version__)
print("torch version        :", torch.__version__)
print("CUDA dispo:", torch.cuda.is_available())

from transformers import AutoModelForCausalLM
from peft import LoraConfig, get_peft_model
import torch

base_model_id = "mistralai/Mistral-7B-Instruct-v0.3"

# Chargement du modÃ¨le en fp16, sans quantization 4/8 bits
model = AutoModelForCausalLM.from_pretrained(
    base_model_id,
    torch_dtype=torch.float16,
    device_map="auto",   # accelerate rÃ©partit sur le GPU (et CPU si besoin)
)

lora_config = LoraConfig(
    r=64,
    lora_alpha=16,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=[
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj",
    ],
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# Bloc 5 â€” SFTTrainer + hyperparamÃ¨tres DO_vert
from trl import SFTTrainer
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./lyra_SO_vert_qLoRA",   # dossier de travail (checkpoints intermÃ©diaires)
    num_train_epochs=3,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    gradient_accumulation_steps=8,
    learning_rate=2e-4,
    fp16=True,
    logging_steps=10,
    eval_strategy="epoch",
    save_strategy="epoch",
    lr_scheduler_type="cosine",
    warmup_ratio=0.03,
    weight_decay=0.0,
    report_to="none",
)

trainer = SFTTrainer(
    model=model,
    processing_class=tokenizer,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    args=training_args,
)

trainer.train()

# Bloc 6 â€” Sauvegarde des adapters dans ./lyra_SO_vert_qLoRA_adapter
output_dir = "./lyra_DO_vert_qLoRA_adapter"

trainer.model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)

print("Adapters sauvegardÃ©s dans", output_dir)
!ls -R {output_dir}

# Bloc 7 â€” CrÃ©ation dâ€™un petit README local (optionnel, Ã  Ã©diter ensuite)
readme_path = f"{output_dir}/README.md"

readme_text = """# lyra_DO_vert_mistral7B_qLoRA

Adapters qLoRA pour un modÃ¨le Mistral-7B-Instruct-v0.3 spÃ©cialisÃ© dans l'analyse de la nutrition azotÃ©e de la tomate Ã  partir de :

- NO3- du sol (mg/kg)
- pH eau
- DO verte (densitÃ© optique de la feuille scannÃ©e)
- Diagnostic : deficit_N_fort, deficit_N_leger, N_normal, excÃ¨s_N_leger, excÃ¨s_N_fort
- Estimation de SPAD Ã©quivalent
- Prise en compte des cas de chlorose ferrique (pH Ã©levÃ©) et de stress non azotÃ© (stress hydrique, maladie, parasitisme)
- Conseils de lutte intÃ©grÃ©e en cas de DO Ã©levÃ©e.

Ce dÃ©pÃ´t contient uniquement les matrices d'adaptation (LoRA adapters)."""

with open(readme_path, "w", encoding="utf-8") as f:
    f.write(readme_text)

print("README local crÃ©Ã© :", readme_path)

# Bloc 8 â€” Push des matrices sur Hugging Face
from huggingface_hub import HfApi, upload_folder, whoami

api = HfApi()
repo_name = "lyra_DO_vert_mistral7B_qLoRA"
full_repo_id = f"jeromex1/{repo_name}"

print("ğŸ‘¤ Compte connectÃ© :", whoami()["name"])

# CrÃ©ation (ou rÃ©utilisation) du repo
api.create_repo(
    repo_id=full_repo_id,
    token=os.environ["HF_TOKEN"],
    private=False,
    exist_ok=True,
)

# Upload du dossier contenant les adapters + README
upload_folder(
    repo_id=full_repo_id,
    folder_path=output_dir,  # ./lyra_SO_vert_qLoRA_adapter
    commit_message="Initial qLoRA adapters for lyra_DO_vert (Mistral-7B-Instruct-v0.3) + model card",
    token=os.environ["HF_TOKEN"],
)

print(f"ğŸš€ Adapters publiÃ©s sur : https://huggingface.co/{full_repo_id}")

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch

# modÃ¨le sur Hugging Face
base_model = "mistralai/Mistral-7B-Instruct-v0.3"
adapter_model = "jeromex1/lyra_DO_vert_mistral7B_qLoRA"

tokenizer = AutoTokenizer.from_pretrained(base_model)

model = AutoModelForCausalLM.from_pretrained(
    base_model,
    torch_dtype=torch.float16,
    device_map="auto",
)

model.load_adapter(adapter_model)
pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=512,
    temperature=0.2,
    top_p=0.9,
)


# --- 5 PROMPTS DE TEST ----------------------------

tests = [
    # 1) Cas normal
    """Culture : tomate
Stade : vÃ©gÃ©tatif
NO3- (sol) : 48.2 mg/kg
pH (eau) : 6.8
DO verte (scanner) : 354
Merci de me donner un diagnostic dÃ©taillÃ©.""",

    # 2) Cas dÃ©ficit
    """Culture : tomate
Stade : vÃ©gÃ©tatif
NO3- (sol) : 12.4 mg/kg
pH (eau) : 6.5
DO verte (scanner) : 275
Merci de me donner un diagnostic dÃ©taillÃ©.""",

    # 3) Cas excÃ¨s
    """Culture : tomate
Stade : fructification
NO3- (sol) : 165 mg/kg
pH (eau) : 7.2
DO verte (scanner) : 510
Merci de me donner un diagnostic dÃ©taillÃ©.""",

    # 4) Cas atypique Â· stress non-azotÃ©
    """Culture : tomate
Stade : floraison
NO3- (sol) : 85 mg/kg
pH (eau) : 7.4
DO verte (scanner) : 268
Merci de me donner un diagnostic dÃ©taillÃ©.""",

    # 5) Cas atypique Â· chlorose ferrique (pH > 7.8)
    """Culture : tomate
Stade : vÃ©gÃ©tatif
NO3- (sol) : 92 mg/kg
pH (eau) : 8.05
DO verte (scanner) : 271
Merci de me donner un diagnostic dÃ©taillÃ©."""
]


# --- INFÃ‰RENCE -------------------------------------

for i, prompt in enumerate(tests, 1):
    print("="*80)
    print(f"ğŸ§ª TEST {i}")
    print("- Prompt:")
    print(prompt)
    print("- RÃ©ponse:")
    out = pipe(prompt)[0]["generated_text"]
    print(out)
    print()